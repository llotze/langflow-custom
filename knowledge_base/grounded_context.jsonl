{"category": "amazon", "name": "s3bucketuploader", "display_name": "S3 Bucket Uploader", "description": "Uploads files to S3 bucket.", "base_classes": ["NoneType"], "inputs": [{"name": "data_inputs", "display_name": "Data Inputs", "type": "other", "info": "The data to split.", "required": true}, {"name": "aws_access_key_id", "display_name": "AWS Access Key ID", "type": "str", "info": "AWS Access key ID.", "required": true}, {"name": "aws_secret_access_key", "display_name": "AWS Secret Key", "type": "str", "info": "AWS Secret Key.", "required": true}, {"name": "bucket_name", "display_name": "Bucket Name", "type": "str", "info": "Enter the name of the bucket.", "required": false}, {"name": "s3_prefix", "display_name": "S3 Prefix", "type": "str", "info": "Prefix for all files.", "required": false}, {"name": "strategy", "display_name": "Strategy for file upload", "type": "str", "info": "Choose the strategy to upload the file. By Data means that the source file is parsed and stored as LangFlow data. By File Name means that the source file is uploaded as is.", "required": false}, {"name": "strip_path", "display_name": "Strip Path", "type": "bool", "info": "Removes path from file path.", "required": true}]}
{"category": "amazon", "name": "AmazonBedrockModel", "display_name": "Amazon Bedrock", "description": "Generate text using Amazon Bedrock LLMs with the legacy ChatBedrock API. For better compatibility, newer features, and improved conversation handling, we recommend using Amazon Bedrock Converse instead.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "aws_access_key_id", "display_name": "AWS Access Key ID", "type": "str", "info": "The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.", "required": true}, {"name": "aws_secret_access_key", "display_name": "AWS Secret Access Key", "type": "str", "info": "The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.", "required": true}, {"name": "aws_session_token", "display_name": "AWS Session Token", "type": "str", "info": "The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.", "required": false}, {"name": "credentials_profile_name", "display_name": "Credentials Profile Name", "type": "str", "info": "The name of the profile to use from your ~/.aws/credentials file. If not provided, the default profile will be used.", "required": false}, {"name": "endpoint_url", "display_name": "Endpoint URL", "type": "str", "info": "The URL of the Bedrock endpoint to use.", "required": false}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "model_id", "display_name": "Model ID", "type": "str", "info": "List of available model IDs to choose from.", "required": false}, {"name": "model_kwargs", "display_name": "Model Kwargs", "type": "dict", "info": "Additional keyword arguments to pass to the model.", "required": false}, {"name": "region_name", "display_name": "Region Name", "type": "str", "info": "The AWS region where your Bedrock resources are located.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}]}
{"category": "amazon", "name": "AmazonBedrockConverseModel", "display_name": "Amazon Bedrock Converse", "description": "Generate text using Amazon Bedrock LLMs with the modern Converse API for improved conversation handling. We recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "additional_model_fields", "display_name": "Additional Model Fields", "type": "dict", "info": "Additional model-specific parameters for fine-tuning behavior.", "required": false}, {"name": "aws_access_key_id", "display_name": "AWS Access Key ID", "type": "str", "info": "The access key for your AWS account. Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.", "required": true}, {"name": "aws_secret_access_key", "display_name": "AWS Secret Access Key", "type": "str", "info": "The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.", "required": true}, {"name": "aws_session_token", "display_name": "AWS Session Token", "type": "str", "info": "The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.", "required": false}, {"name": "credentials_profile_name", "display_name": "Credentials Profile Name", "type": "str", "info": "The name of the profile to use from your ~/.aws/credentials file. If not provided, the default profile will be used.", "required": false}, {"name": "disable_streaming", "display_name": "Disable Streaming", "type": "bool", "info": "If True, disables streaming responses. Useful for batch processing.", "required": false}, {"name": "endpoint_url", "display_name": "Endpoint URL", "type": "str", "info": "The URL of the Bedrock endpoint to use.", "required": false}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_tokens", "display_name": "Max Tokens", "type": "int", "info": "Maximum number of tokens to generate.", "required": false}, {"name": "model_id", "display_name": "Model ID", "type": "str", "info": "List of available model IDs to choose from.", "required": false}, {"name": "region_name", "display_name": "Region Name", "type": "str", "info": "The AWS region where your Bedrock resources are located.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "float", "info": "Controls randomness in output. Higher values make output more random.", "required": false}, {"name": "top_k", "display_name": "Top K", "type": "int", "info": "Limits the number of highest probability vocabulary tokens to consider. Note: Not all models support top_k. Use 'Additional Model Fields' for manual configuration if needed.", "required": false}, {"name": "top_p", "display_name": "Top P", "type": "float", "info": "Nucleus sampling parameter. Controls diversity of output.", "required": false}]}
{"category": "groq", "name": "GroqModel", "display_name": "Groq", "description": "Generate text using Groq.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "api_key", "display_name": "Groq API Key", "type": "str", "info": "API key for the Groq API.", "required": false}, {"name": "base_url", "display_name": "Groq API Base", "type": "str", "info": "Base URL path for API requests, leave blank if not using a proxy or service emulator.", "required": false}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_tokens", "display_name": "Max Output Tokens", "type": "int", "info": "The maximum number of tokens to generate.", "required": false}, {"name": "model_name", "display_name": "Model", "type": "str", "info": "The name of the model to use.", "required": false}, {"name": "n", "display_name": "N", "type": "int", "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].", "required": false}, {"name": "tool_model_enabled", "display_name": "Enable Tool Models", "type": "bool", "info": "Select if you want to use models that can work with tools. If yes, only those models will be shown.", "required": false}]}
{"category": "amazon", "name": "AmazonBedrockEmbeddings", "display_name": "Amazon Bedrock Embeddings", "description": "Generate embeddings using Amazon Bedrock models.", "base_classes": ["Embeddings"], "inputs": [{"name": "aws_access_key_id", "display_name": "AWS Access Key ID", "type": "str", "info": "The access key for your AWS account.Usually set in Python code as the environment variable 'AWS_ACCESS_KEY_ID'.", "required": true}, {"name": "aws_secret_access_key", "display_name": "AWS Secret Access Key", "type": "str", "info": "The secret key for your AWS account. Usually set in Python code as the environment variable 'AWS_SECRET_ACCESS_KEY'.", "required": true}, {"name": "aws_session_token", "display_name": "AWS Session Token", "type": "str", "info": "The session key for your AWS account. Only needed for temporary credentials. Usually set in Python code as the environment variable 'AWS_SESSION_TOKEN'.", "required": false}, {"name": "credentials_profile_name", "display_name": "Credentials Profile Name", "type": "str", "info": "The name of the profile to use from your ~/.aws/credentials file. If not provided, the default profile will be used.", "required": false}, {"name": "endpoint_url", "display_name": "Endpoint URL", "type": "str", "info": "The URL of the AWS Bedrock endpoint to use.", "required": false}, {"name": "model_id", "display_name": "Model Id", "type": "str", "info": "", "required": false}, {"name": "region_name", "display_name": "Region Name", "type": "str", "info": "The AWS region where your Bedrock resources are located.", "required": false}]}
{"category": "huggingface", "name": "HuggingFaceModel", "display_name": "Hugging Face", "description": "Generate text using Hugging Face Inference APIs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "custom_model", "display_name": "Custom Model ID", "type": "str", "info": "Enter a custom model ID from Hugging Face Hub", "required": true}, {"name": "huggingfacehub_api_token", "display_name": "HuggingFace HubAPI Token", "type": "str", "info": "", "required": true}, {"name": "inference_endpoint", "display_name": "Inference Endpoint", "type": "str", "info": "Custom inference endpoint URL.", "required": true}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_new_tokens", "display_name": "Max New Tokens", "type": "int", "info": "Maximum number of generated tokens", "required": false}, {"name": "model_id", "display_name": "Model ID", "type": "str", "info": "Select a model from Hugging Face Hub", "required": true}, {"name": "model_kwargs", "display_name": "Model Keyword Arguments", "type": "dict", "info": "", "required": false}, {"name": "repetition_penalty", "display_name": "Repetition Penalty", "type": "float", "info": "The parameter for repetition penalty. 1.0 means no penalty.", "required": false}, {"name": "retry_attempts", "display_name": "Retry Attempts", "type": "int", "info": "", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "task", "display_name": "Task", "type": "str", "info": "The task to call the model with. Should be a task that returns `generated_text` or `summary_text`.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "The value used to module the logits distribution", "required": false}, {"name": "top_k", "display_name": "Top K", "type": "int", "info": "The number of highest probability vocabulary tokens to keep for top-k-filtering", "required": false}, {"name": "top_p", "display_name": "Top P", "type": "float", "info": "If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation", "required": false}, {"name": "typical_p", "display_name": "Typical P", "type": "float", "info": "Typical Decoding mass.", "required": false}]}
{"category": "azure", "name": "AzureOpenAIModel", "display_name": "Azure OpenAI", "description": "Generate text using Azure OpenAI LLMs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "api_key", "display_name": "Azure Chat OpenAI API Key", "type": "str", "info": "", "required": true}, {"name": "api_version", "display_name": "API Version", "type": "str", "info": "", "required": false}, {"name": "azure_deployment", "display_name": "Deployment Name", "type": "str", "info": "", "required": true}, {"name": "azure_endpoint", "display_name": "Azure Endpoint", "type": "str", "info": "Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`", "required": true}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_tokens", "display_name": "Max Tokens", "type": "int", "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.", "required": false}]}
{"category": "anthropic", "name": "AnthropicModel", "display_name": "Anthropic", "description": "Generate text using Anthropic's Messages API and models.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "api_key", "display_name": "Anthropic API Key", "type": "str", "info": "Your Anthropic API key.", "required": true}, {"name": "base_url", "display_name": "Anthropic API URL", "type": "str", "info": "Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.", "required": false}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_tokens", "display_name": "Max Tokens", "type": "int", "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "required": false}, {"name": "model_name", "display_name": "Model Name", "type": "str", "info": "", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].", "required": false}, {"name": "tool_model_enabled", "display_name": "Enable Tool Models", "type": "bool", "info": "Select if you want to use models that can work with tools. If yes, only those models will be shown.", "required": false}]}
{"category": "vertexai", "name": "VertexAiModel", "display_name": "Vertex AI", "description": "Generate text using Vertex AI LLMs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "credentials", "display_name": "Credentials", "type": "file", "info": "JSON credentials file. Leave empty to fallback to environment variables", "required": false}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "location", "display_name": "Location", "type": "str", "info": "", "required": false}, {"name": "max_output_tokens", "display_name": "Max Output Tokens", "type": "int", "info": "", "required": false}, {"name": "max_retries", "display_name": "Max Retries", "type": "int", "info": "", "required": false}, {"name": "model_name", "display_name": "Model Name", "type": "str", "info": "", "required": false}, {"name": "project", "display_name": "Project", "type": "str", "info": "The project ID.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "float", "info": "", "required": false}, {"name": "top_k", "display_name": "Top K", "type": "int", "info": "", "required": false}, {"name": "top_p", "display_name": "Top P", "type": "float", "info": "", "required": false}, {"name": "verbose", "display_name": "Verbose", "type": "bool", "info": "", "required": false}]}
{"category": "cohere", "name": "CohereModel", "display_name": "Cohere Language Models", "description": "Generate text using Cohere LLMs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "cohere_api_key", "display_name": "Cohere API Key", "type": "str", "info": "The Cohere API Key to use for the Cohere model.", "required": true}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.", "required": false}]}
{"category": "openai", "name": "OpenAIModel", "display_name": "OpenAI", "description": "Generates text using OpenAI LLMs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "api_key", "display_name": "OpenAI API Key", "type": "str", "info": "The OpenAI API Key to use for the OpenAI model.", "required": true}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "json_mode", "display_name": "JSON Mode", "type": "bool", "info": "If True, it will output JSON regardless of passing a schema.", "required": false}, {"name": "max_retries", "display_name": "Max Retries", "type": "int", "info": "The maximum number of retries to make when generating.", "required": false}, {"name": "max_tokens", "display_name": "Max Tokens", "type": "int", "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "required": false}, {"name": "model_kwargs", "display_name": "Model Kwargs", "type": "dict", "info": "Additional keyword arguments to pass to the model.", "required": false}, {"name": "model_name", "display_name": "Model Name", "type": "str", "info": "", "required": false}, {"name": "openai_api_base", "display_name": "OpenAI API Base", "type": "str", "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "required": false}, {"name": "seed", "display_name": "Seed", "type": "int", "info": "The seed controls the reproducibility of the job.", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "slider", "info": "", "required": false}, {"name": "timeout", "display_name": "Timeout", "type": "int", "info": "The timeout for requests to OpenAI completion API.", "required": false}]}
{"category": "mistral", "name": "MistralModel", "display_name": "MistralAI", "description": "Generates text using MistralAI LLMs.", "base_classes": ["LanguageModel", "Message"], "inputs": [{"name": "api_key", "display_name": "Mistral API Key", "type": "str", "info": "The Mistral API Key to use for the Mistral model.", "required": true}, {"name": "input_value", "display_name": "Input", "type": "str", "info": "", "required": false}, {"name": "max_concurrent_requests", "display_name": "Max Concurrent Requests", "type": "int", "info": "", "required": false}, {"name": "max_retries", "display_name": "Max Retries", "type": "int", "info": "", "required": false}, {"name": "max_tokens", "display_name": "Max Tokens", "type": "int", "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "required": false}, {"name": "mistral_api_base", "display_name": "Mistral API Base", "type": "str", "info": "The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "required": false}, {"name": "model_name", "display_name": "Model Name", "type": "str", "info": "", "required": false}, {"name": "random_seed", "display_name": "Random Seed", "type": "int", "info": "", "required": false}, {"name": "safe_mode", "display_name": "Safe Mode", "type": "bool", "info": "", "required": false}, {"name": "stream", "display_name": "Stream", "type": "bool", "info": "Stream the response from the model. Streaming works only in Chat.", "required": false}, {"name": "system_message", "display_name": "System Message", "type": "str", "info": "System message to pass to the model.", "required": false}, {"name": "temperature", "display_name": "Temperature", "type": "float", "info": "", "required": false}, {"name": "timeout", "display_name": "Timeout", "type": "int", "info": "", "required": false}, {"name": "top_p", "display_name": "Top P", "type": "float", "info": "", "required": false}]}